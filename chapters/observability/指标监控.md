监控指标是系统的“生命体征”——用数字告诉我们系统此刻是健康、发烧，还是已经进 ICU。

它以时间序列的形式记录一切关键状态：CPU 使用率、内存占用、磁盘 I/O、网络流量、请求延迟、错误率……这些数字背后，藏着系统的脉搏和呼吸。

当架构复杂到上百个服务、成千上万条调用链时，监控指标就成了你在系统丛林中的“雷达”。

# 监控指标是可观测性的基石 🧱

**1. 即时反馈系统状态**

监控指标就像体温计——能告诉你系统此刻是不是在发烧。

它能实时呈现系统的健康状态，一旦关键指标出现异常，就能立刻触发预警，让问题在还没“扩散成疫情”之前被发现。

比如延迟飙升、CPU 飙高、请求量暴涨，这些都能第一时间提醒你：“嗨，有点不对劲，赶紧看看！”

**2. 数据驱动决策**

工程师的直觉很重要，但数据更诚实。

监控指标让决策不再凭感觉，而是靠证据。

通过分析这些指标，我们能识别出性能瓶颈、资源浪费点，甚至提前预知可能的系统崩溃。

扩容不再靠“感觉要挂了”，而是基于数据的冷静判断。

**3. 跨层级分析问题**

一个指标往往只是“冰山一角”。

好的监控体系应该能从全局视角下钻到局部细节——从整体吞吐量看到具体节点，从用户体验追到具体服务。

这也是日志、追踪等可观测性手段的基础：指标是线索，日志是故事，追踪是现场视频。三者合在一起，才能拼出真相。

**4. 自动化与报警**

再敬业的人也不能 7×24 小时盯着图表。

自动化报警机制能让你“睡着也有警铃响”。

通过为关键指标设定阈值，系统在出现异常时自动发出告警，帮助团队迅速响应。

当然，报警要精，不然容易变成噪音地狱（相信每个被 PagerDuty 叫醒过的工程师都懂 🥲）。

# 监控指标设计原则：VALET 原则 🚗

VALET 原则是 SRE 和 SLO 领域里常用的一套监控设计框架。

它帮助团队在成百上千个指标里，挑出真正重要的那几个。

VALET 由五个维度组成：**Volume、Availability、Latency、Error、Ticket**。记住它，就能让指标更聚焦用户体验，也更有业务价值。

**1. Volume（容量）**

容量指标描述系统的“承载极限”——能处理多少请求、能支撑多大流量。

这类指标常用于容量规划与峰值预案。

- **示例**：QPS、并发连接数、数据库连接池使用率、集群节点数
- **应用场景**：容量评估、流量建模、扩容决策

> 举个例子：当购物节来临，系统的 QPS 从 10 万跳到 50 万，Volume 指标能帮你提前预测到“洪峰”，从容应对，而不是被拍在沙滩上。
> 

**2. Availability（可用性）**

可用性就是“系统有多靠谱”。

它衡量的是用户请求成功的比例，也反映出整体服务的健康程度。

- **示例**：请求成功率、服务实例存活率、任务成功率
- **应用场景**：SLO 设定、错误预算分配、故障评估

一个高可用系统不是永不出错，而是**出错时用户没察觉**。

如果 99.95% 的请求都成功了，剩下 0.05% 就是你未来几天的修复计划 😅。

**3. Latency（时延）**

时延是用户最敏感的指标。

没人愿意等 3 秒看一个按钮响应。

延迟要靠分位数（比如 P90、P99）来衡量，而不是平均值——否则慢得离谱的那 1% 请求永远被“平均掉”。

- **示例**：接口响应时间、数据库慢查询耗时、任务执行延迟
- **应用场景**：性能优化、瓶颈定位、用户体验评估

如果系统的平均响应是 100ms，但 P99 是 2 秒，那说明大部分人满意，少部分人暴躁。SRE 的任务，就是让这 2 秒变成 0.2 秒。

**4. Error（错误率）**

错误率是“系统健康的红灯”。

包括系统级错误（5xx）和业务逻辑错误（库存不足、验证码错误等）。

错误率的定义要结合上下文，有时候 4xx 也不能忽略。

- **示例**：接口错误率、任务失败率、数据库死锁次数
- **应用场景**：根因分析、错误预算、业务容错设计

错误不是洪水猛兽，关键是**能不能被及时发现、量化、控制**。

**5. Ticket（人工介入）**

Ticket 指标衡量系统需要人帮忙的频率。每一次手动恢复、人工扩容、异常工单，都是系统自动化的漏洞。

- **示例**：人工重启次数、工单量、手动扩容操作数
- **应用场景**：自动化水平评估、SLO 校准、运维效率优化

一个成熟的系统，应该能让人“尽量不插手”。

如果你发现每周都要人工点三次“重启服务”，那不是稳定性，是体力活 😂。

## VALET 原则的价值

1. **帮助团队定义 SLO 指标体系**
    
    VALET 提供了一种结构化方式，帮你为不同服务选出关键指标。
    
    例如某电商平台：
    
    - Volume：平时 QPS ≥ 10 万，大促 ≥ 50 万
    - Availability：请求成功率 ≥ 99.95%
    - Latency：P90 延迟 ≤ 100ms
    - Error：错误率 ≤ 0.1%
    - Ticket：人工干预 ≤ 每月 5 次
2. **让监控从“事后”变成“事前”**
    
    通过 Volume 预测流量洪峰、通过 Error 提前发现异常趋势、通过 Ticket 推动自动化改进。
    
    不再等系统炸了才追日志。
    
3. **让团队有统一语言**
    
    开发、运维、SRE、产品都能围绕同一套 VALET 体系沟通。
    
    当错误预算告急时，大家讨论的不是“我感觉慢”，而是“Latency 明显升高导致 Error 上升”。
    

---

# 实践：如何收集监控指标

### **1. 基于采集代理的方案**

- **Prometheus**：部署 Prometheus 服务器与 Exporter（Node、cAdvisor 等），采集系统和应用指标。
- **TIG Stack（Telegraf + InfluxDB + Grafana）**：Telegraf 采集，InfluxDB 存储，Grafana 展示。
    
    PromQL 和 Flux 语言都非常强大，能让你随手写出“延迟>1s”的报警。
    

### **2. 云平台自带方案**

- **AWS CloudWatch**、**Azure Monitor**、**Google Cloud Monitoring** 等
    
    提供云资源和自定义指标采集能力，天然支持告警、可视化和多账户汇总。
    

### **3. 应用内嵌监控**

- **OpenTelemetry**：业界标准方案。
    
    通过 SDK 直接埋点采集应用的运行数据，并输出日志、指标、追踪信息。
    
- **自定义埋点**：针对业务指标（下单成功率、请求转化率等），自定义采集逻辑上报。

### **4. 从日志中提取指标**

如果业务埋点不完善，可以通过日志分析反推出指标。

利用 **ELK** 或 **Loki**，从日志中解析关键字段（例如响应时间或状态码），生成监控报表或告警规则。

---

# 总结：数字不是目的，是语言 📊

监控指标让系统“说出”自己的状态。

但最终的目标，不是收集更多数字，而是通过这些数字理解背后的故事。

一个成熟的监控体系，既能在问题爆发前提醒你，也能在事后帮你复盘。

就像经验丰富的医生看一眼心电图，就能判断出病因一样——

优秀的指标体系，让你一眼看穿系统在想什么 ❤️‍🔥

# 总结 🧘‍♂️

在分布式系统里，告警像是系统的心跳声。

它能提醒你生命迹象是否正常，也能在心律紊乱时让你提前止损。

但心跳太快，也会让人焦虑。

一旦团队被淹没在告警的噪音里，就会形成一种“狼来了”效应——

真正的告警响起时，大家已经学会了忽略。

告警治理的核心，不是多，而是**准**；

不是要系统“喊得更大声”，而是要它“说得更有意义”。

当你的系统能平静地告诉你：“我有点不舒服，但你不用慌”，

那才是监控体系真正成熟的时刻。

> 稳定性的尽头，不是没有告警，
> 
> 
> 而是每一条告警都值得你起身去看一眼。☕️
>